{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 2\n",
    "Collect some data (e.g., from a friend or your teammate) on the recognition memory task from the\n",
    "slides (or construct your own task) and fit the following two models using Stan:\n",
    "• The One-High-Threshold Model (1HT)\n",
    "• The Two-High-Threshold Model (2HT)\n",
    "The models are depcited on Slide 12 (MPT models). As usual, inspect the convergence of the MCMC\n",
    "samplers and report the estimation results. Do the two models suggest different estimates for the\n",
    "two key parameters? Describe and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "##import seaborn as sns\n",
    "import pandas as pd\n",
    "import pystan as stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    old_new  remember  accuracy\n",
      "0         0         0         1\n",
      "1         0         0         1\n",
      "2         1         1         1\n",
      "3         1         1         1\n",
      "4         0         0         1\n",
      "5         0         0         1\n",
      "6         1         1         1\n",
      "7         0         0         1\n",
      "8         1         1         1\n",
      "9         1         1         1\n",
      "10        0         0         1\n",
      "11        0         0         1\n",
      "12        0         0         1\n",
      "13        1         1         1\n",
      "14        1         0         1\n",
      "15        0         0         1\n",
      "16        1         1         1\n",
      "17        0         1         0\n",
      "18        1         1         1\n",
      "19        1         1         1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('MRT_Data.csv', delimiter=';',dtype={'old_new': int, 'choice': int, 'accuracy': int})\n",
    "##print for debug\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHT_program_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N; // Number of data points\n",
    "    int<lower=0, upper=1> is_old[N]; // Indicator for old (1) or new (0) word\n",
    "    int<lower=0, upper=1> is_confident[N]; // Indicator for confident (1) or not (0)\n",
    "    int<lower=0, upper=1> is_correct[N]; // Indicator for correct (1) or wrong (0) response\n",
    "}\n",
    "parameters {\n",
    "    real mu;        // Mean\n",
    "    real<lower=0> sigma; // Standard deviation\n",
    "    real<lower=0, upper=1> pho; // Probability of remembering a studied item on the old list\n",
    "    real<lower=0, upper=1> gamma; // Probability of guessing old when no memory\n",
    "    \n",
    "}\n",
    "model {\n",
    "    //prior distributions\n",
    "    pho ~ beta(1,1);\n",
    "    gamma ~ beta(1,1);\n",
    "    for (i in 1:N) {\n",
    "        if (is_old[i] == 1&&is_correct[i]==1) {\n",
    "            is_confident[i] ~ bernoulli(pho);\n",
    "            if(is_confident[i]==0){1~bernoulli(gamma);} \n",
    "        }\n",
    "        else{\n",
    "            if(is_correct[i]==1){0~bernoulli(gamma);}\n",
    "            else{1~bernoulli(gamma);}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TwoHT_program_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N; // Number of data points\n",
    "    int<lower=0, upper=1> is_old[N]; // Indicator for old (1) or new (0) word\n",
    "    int<lower=0, upper=1> is_confident[N]; // Indicator for confident (1) or not (0)\n",
    "    int<lower=0, upper=1> is_correct[N]; // Indicator for correct (1) or wrong (0) response\n",
    "}\n",
    "parameters {\n",
    "    real mu;        // Mean\n",
    "    real<lower=0> sigma; // Standard deviation\n",
    "    real<lower=0, upper=1> pho; // Probability of remembering a studied item\n",
    "    real<lower=0, upper=1> gamma; // Probability of guessing old when no memory\n",
    "}\n",
    "model {\n",
    "//prior distributions\n",
    "    pho ~ beta(1,1);\n",
    "    gamma ~ beta(1,1);\n",
    "    for (i in 1:N) {\n",
    "        if(is_correct[i]==1){\n",
    "            is_confident[i] ~ bernoulli(pho);\n",
    "            if(is_confident[i]==0){\n",
    "                if(is_old[i]==1){\n",
    "                    1~bernoulli(gamma);\n",
    "                }\n",
    "                else{0~bernoulli(gamma);}\n",
    "            }\n",
    "        }\n",
    "        else{\n",
    "            0~bernoulli(pho);\n",
    "            if(is_old[i]==1){0~bernoulli(gamma);}\n",
    "            else{1~bernoulli(gamma);}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_1e7208daf67add1d04f08e2ac216baa9 NOW.\n",
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_f28819db432812417a658dcf31f1b9b3 NOW.\n"
     ]
    }
   ],
   "source": [
    "OneHT_model = stan.StanModel(model_code=OneHT_program_code)\n",
    "TwoHT_model = stan.StanModel(model_code=TwoHT_program_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data preparation\n",
    "data={\n",
    "    'N':len(df),\n",
    "    'is_old':df['old_new'],\n",
    "    'is_confident':df['remember'],\n",
    "    'is_correct': df['accuracy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:7522 of 8000 iterations ended with a divergence (94 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.99 to remove the divergences.\n",
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:6398 of 8000 iterations ended with a divergence (80 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.99 to remove the divergences.\n",
      "WARNING:pystan:1006 of 8000 iterations saturated the maximum tree depth of 10 (12.6 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_1e7208daf67add1d04f08e2ac216baa9.\n",
      "4 chains, each with iter=4000; warmup=2000; thin=1; \n",
      "post-warmup draws per chain=2000, total post-warmup draws=8000.\n",
      "\n",
      "        mean se_mean     sd    2.5%     25%     50%     75%   97.5%  n_eff   Rhat\n",
      "mu     1.3e4   6.7e4  1.2e5  -2.6e5   -3218  8090.5   3.6e4   2.5e5      3   3.27\n",
      "sigma    inf     nan    inf 5.1e306 4.6e307 9.2e307 1.4e308 1.8e308    nan    nan\n",
      "pho     0.84  4.0e-3    0.1    0.59    0.78    0.86    0.91    0.98    644    1.0\n",
      "gamma   0.23  4.1e-3   0.11    0.05    0.15    0.22     0.3    0.48    733    1.0\n",
      "lp__  695.33    0.06    1.4  691.84   694.6  695.63  696.38  697.13    514    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Fri Apr 12 00:01:30 2024.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_f28819db432812417a658dcf31f1b9b3.\n",
      "4 chains, each with iter=4000; warmup=2000; thin=1; \n",
      "post-warmup draws per chain=2000, total post-warmup draws=8000.\n",
      "\n",
      "        mean se_mean     sd    2.5%     25%     50%     75%   97.5%  n_eff   Rhat\n",
      "mu     2.9e5   3.0e5  6.3e5  -2.3e5  -2.2e4  120.71   3.0e5   2.2e6      4   2.51\n",
      "sigma    inf     nan    inf 4.9e306 4.8e307 9.4e307 1.4e308 1.8e308    nan    nan\n",
      "pho     0.45  4.1e-3   0.11    0.26    0.38    0.45    0.53    0.66    675    1.0\n",
      "gamma   0.23  4.9e-3   0.11    0.06    0.15    0.22     0.3    0.49    540    1.0\n",
      "lp__  685.59    0.06   1.38  682.04  684.88  685.92  686.61  687.31    617    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Fri Apr 12 00:01:46 2024.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ken12\\anaconda3\\envs\\newstan_env\\lib\\site-packages\\numpy\\core\\_methods.py:179: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "c:\\Users\\ken12\\anaconda3\\envs\\newstan_env\\lib\\site-packages\\numpy\\core\\_methods.py:212: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
     ]
    }
   ],
   "source": [
    "OneHT_fit = OneHT_model.sampling(data=data, chains=4, iter=4000,control={'adapt_delta': 0.99})\n",
    "TwoHT_fit = TwoHT_model.sampling(data=data, chains=4, iter=4000,control={'adapt_delta': 0.99})\n",
    "\n",
    "#convergence inspection and estimations\n",
    "print(OneHT_fit)\n",
    "print(TwoHT_fit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Discussion\n",
    "While not the prettiest results, I never the less achieved reasonable convergence on my models, as shown by the 1.0 rhat results above. The results are unsurprising. For the One High Threshold Model, the data I collected suggested a 98% probability for a participant confidently remembering a word for the old list, and a 48% chance that they would guess that word was from the old list. With the Two-High model, gamma remained unchanged, which makes sense given that the parameter is measuring the liklihood of the same event. Meanwhile pho now represents the probability of a correct confident response, and has fallen to 66%. I imagine my data is to blame, as my subject was unconfident remembering every word from the new list (Results like this may also indicate a lack of clarity in my instruction). I would be interested to see what these results would be like if I clarified my instruction and/or used different subjects, so I will be repeating this experiment, with some additional changes involving a distraction task, for my cognitive psych take home assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
